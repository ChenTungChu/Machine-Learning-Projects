\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm,amsfonts,latexsym,bbm,xspace,graphicx,float,mathtools,
verbatim, xcolor} 
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\newcommand{\new}[1]{\textcolor{red}{#1}}
%\usepackage{psfig}

\newcommand{\future}[1]{\textcolor{red}{#1}}

\newcommand{\hP}{\hat P}
\newcommand{\hp}{\hat p}

\newcommand{\Dk}{\Delta_k}
\newcommand{\Px}{P(x)}
\newcommand{\Qx}{Q(x)}
\newcommand{\Nx}{N_x}

\newcommand{\Py}{P(y)}
\newcommand{\Qy}{Q(y)}
\newcommand{\Pml}{P_{ML}}
\newcommand{\Pmlx}{\Pml(x)}
\newcommand{\Pbeta}{P_{\beta}}
\newcommand{\Pbetax}{\Pbeta(x)}


\newcommand{\dTV}[2]{d_{TV} (#1,#2)}
\newcommand{\dKL}[2]{D(#1||#2)}
\newcommand{\chisq}[2]{\chi^2(#1,#2)}
\newcommand{\eps}{\varepsilon}

\newcommand{\nPepsp}[1]{n^*(#1, \eps)}
\newcommand{\nPeps}{\nPepsp{\cP}}


\newcommand{\sumX}{\sum_{x\in\cX}}

\newcommand{\Bpr}[1]{Bern(#1)}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\input{./glodef} 

\title{Assignment Three\\ ECE 4200/5420, Fall 2021}
\date{}

\begin{document}
\maketitle 

\begin{itemize}
\item
Provide credit to \textbf{any sources} other than the course staff that helped you solve the problems. This includes \textbf{all students} you talked to regarding the problems. 	
\item
You can look up definitions/basics online (e.g., wikipedia, stack-exchange, etc)
\item
{\bf The due date is 10/01/2021, 23.59.59 Eastern time}. 
\item
Submission rules are the same as previous assignments.
\end{itemize}



\begin{problem}{1 (10 points)}
Recall that a linear classifier is specified by $(\vec{w},t)$, where $\vec{w}\in\RR^d$, and $t\in\RR$. The decision rule for a feature vector $ X\in\RR^d$ is given by $\vec{w}\cdot {X} \lessgtr t$ .  

Recall the perceptron algorithm from the lectures. Suppose $d=2$,  $n=4$, and a dataset with four examples:
\begin{center}
  \begin{tabular}{| c | c | c |}
 \hline
$i$ &  feature $ X_i$ & label $y_i$\\
 \hline
1 & $(-0.6, 1)$& $-1$\\
2 & $(-3, -4)$& $-1$\\
3& $(3, -2)$& $+1$\\
4&  (0.5, 1)& +1\\\hline
  \end{tabular}
\end{center}

\begin{enumerate}
	\item 
In class we started with the the initial $(\vec w, t)=(\vec 0,0)$, and derived the convergence of perceptron. In this problem:
\begin{itemize}
\item
Start with the initialization $(\vec w, t)= ( (0,1), 0)$ (the $x$-axis).
\item
Implement the perceptron algorithm \textbf{by hand}. Go over the data-points \textbf{in order}. Output a table of the form below where each row works with one example in some iteration. We have filled in some entries in the first two rows. You need to add rows until no mistakes happen on any example.
\begin{center}
  \begin{tabular}{ |l |l | l | l | l | l | l| }
    \hline
     starting $\vec w$  & starting $t$  & features $X_i$& label $y_i$ & predicted label & new $\vec w$ & new $t$ \\ \hline
     $(0,1)$ & 0&  $(-0.6, 1)$ & $-1$ & $\ldots$ & $\ldots$ &$\ldots$ \\ \hline            $\ldots$ & $\ldots$ &$(-3, -4)$ & $-1$ & $\ldots$ & $\ldots$& $\ldots$\\ \hline 
          $\ldots$ &$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$\\ \hline 
  \end{tabular}
\end{center}
\item
Draw a 2-D grid. On this grid, mark the four examples (like we do on the board). Draw the line you obtain as the final result. 
\end{itemize}
\end{enumerate}
\end{problem}

\begin{problem}{ 2. (15 points)}
Consider the same set-up as in perceptron, where the features all satisfy $\|X_i \|\le 1$, and the training examples are separable with margin $\gamma$. We showed in class that perceptron converges with at most $4/\gamma^2$ updates when initialized to the all zero vector, namely to $(\vec 0, 0)$. Suppose instead the initial start with some initial $(\vec {w_0}, t_0)$ with $\lvert\lvert \vec {w_0}\rvert \rvert \le R$, and $|t_0|\le R$. We run the perceptron algorithm with this initialization. Suppose, $(\vec {w_j}, t_j)$ is the hyperplane after $j$th update.


Let $(\vec{w}_{opt}, t_{opt})$ be the optimal hyperplane with margin $\gamma$ with $\|\vec{w}_{opt}\|^2 = 1$
\begin{enumerate}
\item 
Similar to the class, show that $|t_{opt}| \le 1$, and therefore conclude that 
\[
\|\vec{w}_{opt}\|^2+|t_{opt}|^2 \le 2. 
\]
\item Show that 
\[
(\vec{w}_{j}, t_{j})\cdot (\vec{w}_{opt}, t_{opt})\ge j\gamma-2R.
\]
\item Show that 
\[
(\vec{w}_{j}, t_{j})\cdot (\vec{w}_{j}, t_{j})\le 2j+2R^2.
\]
\item
Using these conclude that the \textbf{total number of updates} before perceptron converges is at most
\[
\frac{4+4R\gamma}{\gamma^2}.
\]
\end{enumerate}

\end{problem}


\begin{problem}{3 (10 points)}
Recall the log-likelihood function for logistic regression:
\[
J(\vec w, t) = \sum_{i=1}^n \log Pr ( y_i|  X_i, \vec w, t), 
\]
where $Pr ( y_i|   X_i, \vec w, t)$ is the same as defined in the class for logistic regression, and $y_i\in\{-1,+1\}$. We will show that this function is concave as a function of $\vec w, t$.
\begin{enumerate}
\item 
Show that for two real numbers $a, b$, $\exp\Paren{a}+\exp\Paren{b}\ge 2 \exp\Paren{(a+b)/2}$. (Basically this says that exponential function is convex.)
\item
Extending this, show that for any vectors $\vec w_1, \vec w_2, \vec x \in\RR^d$, 
\[
\exp\Paren{\vec w_1\cdot \vec x} + \exp\Paren{\vec w_2\cdot \vec x} \ge 2\exp\Paren{\frac{(\vec w_1 + \vec w_2)}2 \cdot \vec x}.
\]
\item
Show that $J(\vec w, t)$ is concave (you can only show the concavity holds for $\lambda=1/2$). You can show it any way you want. One way is to first show that for any $\vec w_1, \vec w_2 \in\RR^d$, and $t_1, t_2\in\RR$,  
\[
\frac{1}{2}J(\vec w_1, t_1)+\frac{1}{2}J(\vec w_2, t_2) \le J\Paren{\frac{\vec w_2+\vec w_2 }2, \frac{t_1+t_2}2}.
\]

You can use that sum of concave functions are concave. A linear function is both concave and convex. 
\end{enumerate}

In this problem you can also work with the vector $\vec w^*$, which is the $d+1$ dimensional vector $(\vec w, t)$, and the $d+1$ dimensional feature vectors $ X_i^{*} = ( X_i, -1)$, which are the features appended with a $-1$. Just like in class, this can simplify some of the computations by using the fact that $\vec w\cdot  X_i -t = \vec w^{*}\cdot  X_i^*$. 
\end{problem}





\begin{problem}{4 (10 points) Different class conditional probabilities}
Consider a classification problem with features in $\RR^d$ and labels in $\{-1, +1\}$. Consider the class of linear classifiers of the form $(\vec w, 0)$, namely all the classifiers (hyper planes) that pass through the origin (i.e., $t=0$). Instead of logistic regression, suppose the class probabilities are given by the following function:
\begin{align}
P\Paren{y=+1|X, \vec w} = \frac12\Paren{1+\frac{\vec w\cdot X}{\sqrt{1+(\vec w\cdot X)^2}}}, 
\end{align}
where $\vec w\cdot X$ is the dot product between $\vec w$ and  $X$. 

Suppose we obtain $n$ independent examples $(X_i, y_i)$ for $i=1,\ldots, n$. 
\begin{enumerate}
\item 
Show that the log-likelihood function is
\begin{align}
J(\vec w) = -n\log 2 + \sum_{i=1}^{n} \log \Paren{1+ \frac{y_i (\vec w\cdot X_i)}{\sqrt{1+(\vec w\cdot X_i)^2}}}.
\end{align}
\item
Compute the gradient of $J(\vec w)$ and write one step of gradient ascent. Namely fill in the blank:
\begin{align}
\vec w_{j+1} = \vec w_j + \eta\cdot \underline{\hspace{3cm}}\nonumber
\end{align}
\textbf{hint:} use the chain rule and $\nabla_{\vec w} \vec w\cdot X = X$. 
\end{enumerate}
\end{problem}


\begin{problem}{5 (25 points)}
See the attached notebook for details.
\end{problem}



\end{document}
